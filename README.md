¬ß# **ü§ñ INTRODUCING TEAM PLADYTRON - WRO 2025**
<div align="center">
  <img width="1280" height="568" alt="image" src="https://github.com/user-attachments/assets/930fa959-45ad-4a6b-877a-23a41c0a6785" />

</div>


## üë• **Team PLADYTRON**


| üë§ Name               | üéì Institution                          | üìö Class/Year       | üéÇ Age  |
|----------------------|-----------------------------------------|---------------------|---------|
| **Anas Bin Azim**    | Adamjee Cantonment College              | Class XII           | 17      |
| **SM Mohiuddin Sami**| Rajuk Uttara Model College              | Class XII           | 17      |

---

**Team Origin**: Bangladesh

---


## üèÖ Achievements

- ü•á **Champion, WRO Bangladesh 2024 (Future Engineers)**
- üåç 20th place **WRO International in Izmir, Turkey**
- üõ† Multiple national awards in robotics, AI, and innovation

---

### üåü **The Meaning Behind PLADYTRON**

Our team name, ‚ÄúPLADYTRON,‚Äù dances between the realms of play and technology‚Äîa symphony of creativity and code. Like sparks igniting in the dark, our ideas come alive through the joyful spirit of discovery and the relentless pulse of innovation. But beyond the screens and circuits, there is an unseen force : our wellspring of strength: the unwavering support of those who walk beside us. Their hopes, whispered like secret blessings, fuel every challenge we embrace. In their quiet faith, we find the courage to dream bigger, to build stronger, and to journey farther. Pladytron is more than a name; it is the heartbeat of a shared dream, where joy and determination intertwine to create something truly extraordinary.

---

# The Team  

## Anas Bin Azim  

<img width="960" height="960" alt="image" src="https://github.com/user-attachments/assets/434a9416-e07d-4d49-8c71-0dfb8161ee08" />


**Age:** 17  

**High School:** Adamjee Cantonment College, Dhaka 

**Description:** Hi! I‚Äôm Anas from Bangladesh and this is my second WRO International season. In 2024, I had the honor of representing Team Mayerdoa Robotics at the World Robot Olympiad in Turkey, where we proudly placed 20th in the Future Engineers category. I‚Äôve been deeply involved in robotics for over 4 years, building autonomous systems and competing in both national and international competitions. Beyond robotics, I‚Äôm passionate about programming, innovation, quizzing, traveling, and playing cricket]  

---

## Mohiuddin Sami  

<img width="960" height="960" alt="image" src="https://github.com/user-attachments/assets/223a0c30-3b4c-477a-b101-ca55ee2e311c" />



**Age:** 17  

**High School:** Rajuk Uttara Model College, Dhaka 

**Description:** Hi! I‚Äôm Sayed Md. Mohiuddin Sami from Rajuk Uttara Model College. I‚Äôve been part of the BDOI camp for 2 years and have spent the last 2 years working in robotics and WRO. In 2024, I had the chance to represent Bangladesh at the World Robot Olympiad International Finals in Turkey. I‚Äôve also been doing competitive programming for 4 years, which has helped me grow in problem-solving and logical thinking. Outside of robotics and coding, I enjoy quizzing, traveling, and exploring new ideas.


## üéâ Project Overview
<img align="right" alt="SMOKI" width="350" src="https://github.com/user-attachments/assets/46c38599-e416-42cb-93ba-6f83ff142c18">

This repository includes all files, designs, and code for **SMOKI**, our WRO 2024 robot. Below is the folder structure:
Here‚Äôs a breakdown of the project folders:

---

## üìë Table of Contents

- [Code](./Code)  
- [Software Setup](./Software-Setup)  
- [WRO 2024 Future Engineers: Self-Driving Project](./WRO-2024-Future-Engineers-Self-Driving...)  
- [Experiments](./experiments)  
- [Model](./model)  
- [Problems and Solutions](./problemsandsolutions)  
- [Schematic](./schematic)  
- [Source (src)](./src)  
- [t-photos](./t-photos)  
- [v-photos](./v-photos)  
- [Video](./video)  
- [README.md](./README.md)  
---




## Robot Photos (All Directions)

<table>
  <tr>
    <td align="center"><b>Front</b></td>
    <td align="center"><b>Back</b></td>
  </tr>
  <tr>
    <td><img width="1346" height="1600" alt="image" src="https://github.com/user-attachments/assets/2e13646d-6dce-4837-b075-077c65d2c4af" />
</td>
    <td><img width="1384" height="1600" alt="image" src="https://github.com/user-attachments/assets/58c12fab-9ce2-4afe-b574-0564f22d38a8" />
</td>
  </tr>
  <tr>
    <td align="center"><b>Top</b></td>
    <td align="center"><b>Bottom</b></td>
  </tr>
  <tr>
    <td><img width="1290" height="1600" alt="image" src="https://github.com/user-attachments/assets/461f2a82-b33d-4397-9bfa-19a6d062d3f5" />
</td>
    <td><img width="1600" height="1526" alt="image" src="https://github.com/user-attachments/assets/7d9cafd1-731f-4eae-8365-433b6c5d03bf" />
</td>
  </tr>
  <tr>
    <td align="center"><b>Left</b></td>
    <td align="center"><b>Right</b></td>
  </tr>
  <tr>
    <td><img width="1600" height="1200" alt="image" src="https://github.com/user-attachments/assets/d4f21dbd-8fda-4921-b1da-b66e4c537d77" />
</td>
    <td><img width="1600" height="1200" alt="image" src="https://github.com/user-attachments/assets/30a29ba1-9319-4936-b94c-d74a8fcea478" />
</td>
  </tr>
</table>


----


---

### Mission Overview for WRO Future Engineers Rounds

<table>
  <tr>
    <td width="50%" valign="top" align="left">
      <h3>üèÅ Round 1: Lap Completion</h3>
      <p>In <strong>Round 1</strong>, the robot must autonomously complete <strong>three laps</strong> on a pre-defined track. The goal of this round is for the bot to demonstrate stable navigation and precise lap tracking without any obstacle avoidance requirements.</p>
      <ul>
        <li><strong>Objective</strong>: Complete three laps on the track within the allotted time.</li>
        <li><strong>Key Tasks</strong>: Accurate path-following, speed control, and lap counting.</li>
      </ul>
      <div align="center">
        <br><br><br><br><br>
        <img src="https://github.com/user-attachments/assets/823b29fa-8c92-479e-a78a-9fc96c407858" alt="Round 1 WRO Track" width="250" height="180" />
      </div>
    </td>
    <td width="50%" valign="top" align="left">
      <h3>üèÜ Round 2: Lap Completion with Obstacle Avoidance and Parking</h3>
      <p>In <strong>Round 2</strong>, the bot must complete <strong>three laps</strong> while avoiding green and red obstacles:</p>
      <ul>
        <li><strong>Green Obstacles</strong>: The bot should move <strong>left</strong> to avoid.</li>
        <li><strong>Red Obstacles</strong>: The bot should move <strong>right</strong> to avoid.</li>
      </ul>
      <p>After completing the laps, the bot must accurately park within a designated zone.</p>
      <ul>
        <li><strong>Objective</strong>: Complete three laps, avoid obstacles, and park in the designated area.</li>
        <li><strong>Tasks</strong>: Obstacle detection, color-based avoidance, and precision parking.</li>
      </ul>
      <div align="center">
        <img src="https://github.com/user-attachments/assets/b578392d-b443-4315-8fe3-f03af828c39a" alt="Round 2 WRO Track" width="250" height="180" />
      </div>
    </td>
  </tr>
</table>

---
>[!IMPORTANT]
>**Important: WRO Future Engineers Rulebook**
>* **Thorough Reading:** Ensure that you thoroughly read the **WRO Future Engineers 2024 Rulebook** to understand all rules and guidelines.
>* **Official Link:** Access the rulebook here: [üîó WRO Future Engineers 2024 Rulebook](https://wro-association.org/competitions/future-engineers/).

---
---


## üß© Components and Hardware
Our bot is equipped with various components that support its autonomous functionality. Here is a breakdown of the key hardware elements used in this project:

| Component                  | Description                                                                                      | Image                          | Purchase Link                                                                                  |
|----------------------------|------------------------------------------------------------------------------------------------|--------------------------------|-----------------------------------------------------------------------------------------------|
| **Chassis - Custom Made**   | Custom-designed chassis tailored to fit all components and optimize stability and movement.    | <div align="center"><img src="" alt="Chassis Custom Made" width="100"></div>         | N/A                                                                                           |
| **RPLIDAR C1**             | 360-degree laser scanner used for mapping and obstacle detection.                              | <div align="center"><img src="" alt="RPLIDAR C1" width="100"></div>                   | [Purchase RPLIDAR C1](https://www.mybotshop.de/SLAMTEC-RPLIDAR-C1-360-Laser-Scanner-12-m)     |
| **Servo Motor SG90**       | Small, lightweight servo motor used for precise control of angles and positioning.             | <div align="center"><img src="" alt="Servo Motor SG90" width="100"></div>             | [Purchase SG90](https://bongotech.ai/product/sg90-micro-servo-motor-180-degree)                |
| **SJ CAM C200**            | Captures visual data, supporting navigation and obstacle detection tasks.                      | <div align="center"><img src="" alt="SJ CAM C200" width="100"></div>                  | [Purchase SJ CAM C200](https://amzn.to/3SJCAM) (From previous knowledge)                      |
| **Buck Module XL4016**     | Provides stable voltage regulation for power management.                                      | <div align="center"><img src="" alt="Buck Module XL4016" width="100"></div>           | [Purchase XL4016](https://amzn.to/4xl4016) (From previous knowledge)                          |
| **Motor Driver L293D**     | Dual H-Bridge motor driver for controlling DC motors and stepper motors.                       | <div align="center"><img src="" alt="Motor Driver L293D" width="100"></div>           | [Purchase L293D](https://electropeak.com/l293d-motor-drive-shield)                            |
| **Booster Module 5V to 40V** | Voltage booster module to step up power supply from 5V to 40V, supporting high-voltage requirements. | <div align="center"><img src="" alt="Booster Module 5V to 40V" width="100"></div>       | [Purchase Booster Module](https://robu.in/product-category/electronic-modules/electronic-module/buck-boost-converter/boost-converter/) |





# Mobility Management :

#### üõ†Ô∏è Chassis Assembly Process

## CHASSIS FIRST FLOOR : 

<img width="1422" height="732" alt="image" src="https://github.com/user-attachments/assets/a67bffe7-39af-4c0c-b009-ef4beefd3101" />

This is the first floor of the robot chassis, designed to serve as the primary structural layer of the bot. The STL model shows a flat base plate with multiple precisely positioned mounting holes and slots for components.

Key Features:

Steering Shaft Mount: The elongated slot at the top center is designed to hold the steering shaft, ensuring smooth and stable movement.

Servo Motor Mount: Dedicated mounting holes allow the servo motor to be securely attached for steering control.

Camera Holder: The central cut-out and surrounding structure provide space for installing a camera for vision-based navigation.

Wheel Mounts: Side holes are aligned to support the wheels, making them properly fixed to the frame.

Base Support: This floor acts as the foundation of the robot, supporting the upper floors and ensuring stability during operation.

Dimensions:

Width (X): 101.5 mm

Length (Y): 185.0 mm

Thickness (Z): 10.0 mm

---
---





### CHASSIS STEERING :
<img width="1390" height="721" alt="image" src="https://github.com/user-attachments/assets/5bf6bf24-4169-486a-9a18-3f7ea01eafa4" />



This is the steering plate of the robot chassis, designed to integrate with the first floor and provide precise steering control. The central rectangular gap is a servo clearance slot, which allows the servo horn and linkage to move freely during steering. The smaller holes around the center are meant for mounting the servo securely, while the larger side holes provide attachment points to connect the steering plate with the rest of the chassis. The plate is compact yet durable, ensuring stability during turns, and it directly connects the servo motor to the steering shaft and wheels, making it an essential component for controlling the robot‚Äôs movement.

Dimensions:

Width (X): 96.2 mm

Height (Y): 35.55 mm

Thickness (Z): 10 mm

---
---

### STEERING SHAFT :

<img width="1394" height="725" alt="image" src="https://github.com/user-attachments/assets/270cb784-abef-46f4-95c1-31577cfeb1a9" />

This part is the steering shaft of the robot chassis. It connects directly to the steering plate and transmits the motion from the servo motor to the wheels, enabling controlled directional movement. The design includes a central body with mounting cut-outs and side holes that allow it to be fixed securely to the chassis while maintaining free rotational movement where needed. Its sturdy build ensures that torque from the servo is effectively transferred to the wheels without bending or slipping, making it a key component for precise steering.

Dimensions:

Width (X): 

Height (Y): 

Thickness (Z): 


---
---


## CHASSIS SECOND FLOOR :
<img width="1381" height="718" alt="image" src="https://github.com/user-attachments/assets/816acdff-af66-435c-bd58-89c1de20a055" />


This is the second floor of the robot chassis, designed to mount the following components:

RPLIDAR C1 (with a 10 cm clearance ensured by floor orientation)

LiPo 1500mAh 9A 3S battery

STL Dimensions: 131.27 mm √ó 101.50 mm √ó 10.00 mm

The plate includes cutouts for weight reduction and cable management, along with mounting holes for secure installation. Its orientation was carefully adjusted to provide the RPLIDAR with optimal clearance for unobstructed 360¬∞ scanning.

---


## CHASSIS TOP FLOOR:
<img width="1420" height="720" alt="image" src="https://github.com/user-attachments/assets/9e55ee47-a77b-4c2a-9d98-42b5f0e091a5" />

Top & 3rd Floor ‚Äì Electronics Section

This floor of the chassis holds the core electronic components required for power regulation and motor control.

Components

Raspberry Pi 4B ‚Äì Acts as the main controller, handling computation, control logic, and communication.

XL4016 Buck Converter ‚Äì Steps down the LiPo battery voltage to the levels required by the Raspberry Pi and other electronics.

L293D Motor Driver ‚Äì Provides motor control by supplying the necessary current and voltage to the DC motors.

Power Switch ‚Äì Allows safe shutdown and startup of the electronics.

Veroboard ‚Äì Serves as the wiring platform, connecting the Raspberry Pi, buck converter, motor driver, and switch.

Functionality

The LiPo battery powers the system.

The buck converter regulates the voltage for stable Raspberry Pi operation.

The L293D motor driver interfaces between the Raspberry Pi and the motors, enabling directional control.

The veroboard ensures secure and organized wiring between all components.

The switch provides overall power control for the floor.

---
---


>[!IMPORTANT]
> ALL THE STL AND DXF FILES ARE PROVIDED IN THE MODEL SECTION AND A PREVIEW OF THE WHOLE THING COMBINED IS GIVEN BELOW.
---

# TOTAL PREVIEW:

## Model Views

<table>
  <tr>
    <td align="center"><b>Top</b></td>
    <td align="center"><b>Bottom</b></td>
  </tr>
  <tr>
    <td><img width="544" height="831" alt="rsz_iwo" src="https://github.com/user-attachments/assets/3c0d04ae-508e-48ff-982b-fbca49025b40" />

</td>
    <td><img width="544" height="831" alt="bottomviewbot" src="https://github.com/user-attachments/assets/36fe9b4a-93c7-4e48-aeeb-8a4c9824f216" />
</td>
  </tr>
  <tr>
    <td align="center"><b>Left</b></td>
    <td align="center"><b>Right</b></td>
  </tr>
  <tr>
    <td><img width="1481" height="842" alt="leftviewbot" src="https://github.com/user-attachments/assets/a1a89225-2e90-49f4-a9f2-a159ed0b6e42" />
</td>
    <td><img width="1445" height="814" alt="rightviewbot" src="https://github.com/user-attachments/assets/48038688-0663-4524-9830-18823f7520f6" />
</td>
<tr>
    <td align="center"><b>Tilt1</b></td>
    <td align="center"><b>Tilt2</b></td>
  </tr>
  <tr>
    <td><img width="750" height="803" alt="TILTEDIMAGE" src="https://github.com/user-attachments/assets/34c8f2c8-1b38-43ea-bd9b-fc0961c088fa" />
</td>
    <td><img width="750" height="803" alt="TILTEDIMAGE" src="https://github.com/user-attachments/assets/b37f2262-4a0d-4781-af76-2328b026c4da" />

</td>
  </tr>

</table>

---
---

# Mobility Management:

The mobility system of our robot has been meticulously designed to ensure smooth, efficient, and reliable movement, addressing both power distribution and maneuverability. This section outlines the evolution of our robot's mobility systems, including upgrades to the steering and gear systems.

---

#### **16GA 800 RPM DC Gear Motor**

<table>
<tr>
<td width="50%">
<div align="center">
  <img width="600" height="597" alt="image" src="https://github.com/user-attachments/assets/5a446791-2e61-4f52-b6ba-a0e01f6ba39b" />

</div>
</td>
<td width="50%">

A **16GA DC gear motor** is a DC motor with an integrated gearbox. It provides controlled shaft rotation with increased output torque through gearbox reduction.

##### **How the Motor Works**
- The DC motor converts electrical input into rotational motion of the armature and shaft.  
- The gearbox stages reduce output speed and increase torque at the output shaft.  
- The D-flat on the shaft provides a positive mechanical interface for gears, pulleys, or couplings.

##### **Specifications**
- **Rated Speed:** 800 RPM  
- **Shaft Diameter:** 3.0 mm (D-flat: 2.5 mm)  
- **Gearbox Diameter:** 16 mm  
- **Gearbox Length:** 14 mm  
- **Motor Body Length:** 26.5 mm  
- **Shaft Length (protruding):** 11.2 mm

##### **Advantages**
1. **Compact Design**: Small gearbox diameter and short overall length for tight assemblies.  
2. **Increased Torque**: Gear reduction delivers higher torque at the output shaft for loaded conditions.  
3. **Positive Shaft Interface**: D-flat prevents rotary slip when fitted with set-screws or couplings.

</td>
</tr>
</table>

---

### **Rear Axle Power Distribution**

Initially, the rear wheels were powered through a **Bevel Gear**, but we later upgraded to a **Differential Gearbox** to improve efficiency and performance during turns.

---

#### **Bevel Gear**

<table>
<tr>
<td width="50%">
<div align="center">
  <img src="https://github.com/user-attachments/assets/9e19661f-b921-4bea-9028-4e0274306ced" width="300"/>
</div>
</td>
<td width="50%">

A **bevel gear** is a type of gear where the axes of the two shafts intersect, and the tooth-bearing faces of the gears are conical. Bevel gears are commonly used to transfer motion between intersecting shafts at an angle, typically 90¬∞.

##### **How Bevel Gears Work**
- A driver gear transfers motion to a driven gear, which rotates an output shaft.
- The teeth of the gears are designed to mesh smoothly, transferring torque efficiently between the shafts.

##### **Advantages of Bevel Gears**
1. **Compact Design**: Suitable for space-constrained applications.
2. **Efficient Torque Transfer**: Provides reliable power transmission at angles.
3. **Versatility**: Can operate at angles other than 90¬∞ if needed.

</td>
</tr>
</table>

---
>[!IMPORTANT]
> Bevel gears were essential in the initial stages of our design, but they had limitations in terms of energy efficiency during turns.
---

#### **Differential Gearbox**

<table>
<tr>
<td width="50%">

A **differential gearbox** allows the wheels on the same axle to rotate at different speeds while receiving power from a single motor. This is crucial for smooth turning, where the outer wheel must travel a larger distance than the inner wheel.

##### **Advantages of Differential Gearbox**
1. **Smooth Turns**: Adapts to varying wheel speeds, ensuring efficient cornering.
2. **Energy Efficiency**: Reduces energy wastage by minimizing wheel slippage.
3. **Component Longevity**: Minimizes wear on tires and axles.

##### **How Differential Gears Work**
- Power from the motor is delivered to an input shaft.
- The differential splits the torque between the two wheels via bevel or spider gears inside the housing.
- During turns, the differential allows one wheel to spin faster than the other, ensuring smooth movement.

</td>
<td width="50%">
<div align="center">
  <img src="https://github.com/user-attachments/assets/4bd7f40f-d350-4215-8cf4-2bf46d3cc779" width="300"/>
</div>
</td>
</tr>
</table>

---

### **Servo Motor with L293D Motor Driver**

<table>
<tr>
<td width="30%">
<div align="center">
  <img src="https://github.com/user-attachments/assets/ffcfa0ae-2682-4199-8c0b-4bcbff421d08" width="150"/>
</div>
</td>
<td width="70%">

We used the **L293D Motor Driver** in combination with a **Servo Motor** to control the robot's wheels effectively. The L293D is a dual H-Bridge motor driver that is perfectly suited for the LEGO motor, which operates at **750mA** current.

##### **Why L293D?**
- **Optimal Current Capacity**: Can handle up to **1A** peak current, suitable for the LEGO motor‚Äôs 750mA requirement.
- **Bidirectional Control**: Facilitates forward and backward motion of the wheels.
- **Compact and Lightweight**: Ideal for small-scale robotic systems.
- **PWM Support**: Enables smooth speed control of the motors.

##### **Advantages of the Servo Motor with L293D**
1. **Accurate Steering**: The servo motor ensures precise angle adjustments.
2. **Smooth Speed Control**: PWM functionality provides variable speed control.
3. **Efficient Current Management**: Matches the LEGO motor's current needs, ensuring reliable operation.

</td>
</tr>
</table>

---

#### **Comparison: Bevel Gear vs. Differential Gearbox**

| **Feature**                | **Bevel Gear**                            | **Differential Gearbox**                |
|----------------------------|-------------------------------------------|-----------------------------------------|
| **Turning Efficiency**     | Limited; fixed wheel speeds.             | Superior; wheels rotate independently. |
| **Energy Usage**           | Higher due to slippage during turns.     | Lower; optimized for dynamic turns.    |
| **Durability**             | Higher strain on components.             | Reduced strain; longer component life. |

---

### **Steering Systems**

The robot was first designed with an Ackermann Steering System, which is widely recognized for its efficiency in full-scale vehicles where smooth and realistic cornering is important. This system was chosen because it closely replicates how cars steer, making it ideal for projects that aim to simulate real-world vehicle dynamics. However, as the design of the robot progressed, the steering system was later replaced with a LEGO-based steering mechanism. The change was made to simplify construction, reduce complexity, and improve modularity, all while maintaining effective and reliable steering control. The LEGO steering setup makes it easier to experiment, repair, and scale the design without sacrificing too much performance, which is an important advantage in prototyping and educational robotics.

The Ackermann Steering System itself is a clever mechanical arrangement that ensures each wheel follows a natural path while the vehicle turns. In any cornering situation, the inner wheels of a vehicle must rotate at a sharper angle than the outer wheels because they travel along a smaller radius. Without this adjustment, wheels would slip or drag, leading to wasted energy and uneven motion. The Ackermann geometry solves this by connecting the front wheels through steering arms angled in such a way that the imaginary extension of these arms intersects at the center of the turn‚Äôs radius. This alignment ensures that all wheels follow concentric paths, allowing the vehicle to turn smoothly and efficiently without excessive tire wear or unnecessary power loss.

In practice, the Ackermann system has several clear advantages for robotic applications. It minimizes slippage between the wheels and the ground, which results in more efficient turning and better energy conservation. The reduction of wheel drag also means that the motors driving the vehicle consume less power during maneuvers, which is especially valuable in battery-powered robots where energy efficiency extends operating time. Another key benefit is that it provides a highly realistic simulation of how cars steer, making it particularly useful for robotics projects that aim to mimic real-world automotive systems or serve as educational platforms to demonstrate fundamental vehicle dynamics. Although our project ultimately transitioned to a LEGO-based steering approach for modularity and ease of construction, the principles and advantages of Ackermann steering remain highly relevant for anyone interested in understanding efficient and practical steering systems in robotics and beyond.

---
---

## **SERVO MOTOR :** 
# Servo Motor - Tower Pro MG90S

<table>
  <tr>
    <td>
      <img width="613" height="493" alt="image" src="https://github.com/user-attachments/assets/edafe6d4-9394-49ea-980f-ddc946be1b3f" />
    </td>
    <td>
      <h3>Specifications</h3>
      <ul>
        <li><strong>Weight:</strong> 13.4g</li>
        <li><strong>Stall torque:</strong> 2.2 kgf¬∑cm (6V)</li>
        <li><strong>Operating speed:</strong> 0.08 s/60¬∞ (6V)</li>
        <li><strong>Rotation angle:</strong> 120¬∞</li>
        <li><strong>Gear type:</strong> Metal gears</li>
        <li><strong>Operating voltage:</strong> 4.8V ‚Äì 6V</li>
      </ul>
    </td>
  </tr>
</table>

---

The Tower Pro MG90S is a micro servo motor designed for applications where compact size and high torque are required, making it a reliable choice for robotics, RC models, and automation projects. Built with durable metal gears, it offers a longer operational lifespan compared to plastic-geared alternatives, while maintaining a lightweight 13.4g profile that makes it easy to integrate into small-scale designs. Operating on a voltage range of 4.8V to 6V, it delivers up to 2.2 kgf¬∑cm stall torque and a quick 0.08s/60¬∞ response at 6V, ensuring both strength and speed for demanding tasks. With a rotation range of 120¬∞, the MG90S provides precise angular control suitable for robotic arms, pan-tilt camera gimbals, or steering systems in miniature vehicles. Its compatibility with standard PWM signals means it can be driven directly by most microcontrollers, including Arduino and Raspberry Pi, without the need for additional hardware. Engineers and hobbyists often choose the MG90S because it balances performance and durability in a very small form factor, making it a practical and cost-effective solution for projects that require accurate movement and dependable performance under continuous load.

---


## Where to Buy

You can purchase the servo motor here:  
üëâ [Tower Pro MG90S - Cleste.ro](https://cleste.ro/motor-servo-mg90s-180g.html)


## SERVO-HOLDER :

<img width="1024" height="693" alt="image" src="https://github.com/user-attachments/assets/1c2fca76-6a78-4e12-b8b3-385626c73089" />


### SG90 / MG90 Servo Holder

This is a **servo motor holder/mount** designed for **SG90** and **MG90S micro servos**.  
It provides a secure and stable way to fix the servo motor onto different surfaces such as a robot chassis, acrylic base, or wooden frame.

---

## Description
## Servo Motor Holder / Mount (SG90 & MG90S)

This servo motor holder is designed specifically for the popular SG90 and MG90S micro servos, providing a secure and reliable way to mount the motor onto different surfaces such as a robot chassis, acrylic base, or even wooden frames. The primary goal of the mount is to eliminate instability and vibration, ensuring that the servo can deliver accurate and repeatable movements during operation. For small-scale robotics and automation projects where precision matters, this holder becomes an essential accessory.
The base of the mount is a flat rectangular plate equipped with four mounting holes that allow it to be firmly attached using screws or bolts. This rigid foundation ensures that once the holder is installed, it remains stable and does not shift under load, even when the servo is driving linkages or mechanical arms. At the center of the design, there is a servo housing slot carefully dimensioned to snugly fit SG90 and MG90S servos. This slot prevents any side-to-side play and keeps the servo perfectly aligned, which is important when the servo is used for steering mechanisms, grippers, or robotic arms where accuracy of angular motion is critical.
On the sides of the housing are clamp sections with additional screw holes, which allow the servo to be tightly locked in place once inserted. This design makes sure the servo remains fixed during continuous operation, preventing unwanted movement that could lead to mechanical inaccuracy or wear. At the front, the mount features an open clearance space that allows the servo horn or arm to rotate freely without obstruction. This ensures that the servo‚Äôs full range of motion can be utilized, whether it is controlling wheels, levers, or pan-tilt systems.

The bottom of the mount is left open to provide space for the servo‚Äôs wiring to pass through. This prevents cable bending and stress at the connector, reducing the risk of damage to the wires over time and allowing easy integration into compact robotic builds. By providing both mechanical stability and practical cable management, the holder is designed to extend the lifespan of the servo and improve overall reliability of the system.

In terms of material, PLA is often sufficient for hobby projects where light loads and indoor conditions are expected. However, for applications requiring greater durability or exposure to outdoor environments, PETG or ABS is recommended since both materials offer improved toughness and resistance to stress compared to PLA. The mount‚Äôs design is fully compatible with Tower Pro SG90 and MG90S servos, making it versatile for use across a wide range of robotic and automation projects where these micro servos are commonly deployed.


---
---
# ‚ö° Power and Sense Management

The **Power and Sense Management** system of our robot has been meticulously designed to optimize performance while ensuring reliable power delivery, precise sensing, and efficient communication between components.
# Battery: Gens Ace Airsoft 3S Li-Po Battery (11.1V 1500mAh 35C)

<table>
  <tr>
    <td>
      <img width="691" height="515" alt="image" src="https://github.com/user-attachments/assets/c00bcd1d-fe7d-433a-b076-1cb49fd29212" />
    </td>
    <td>
      <h3>Specifications</h3>
      <ul>
        <li><strong>Brand:</strong> Gens Ace Airsoft</li>
        <li><strong>Battery Type:</strong> Li-Po (Lithium Polymer)</li>
        <li><strong>Nominal Voltage:</strong> 11.1V (3S)</li>
        <li><strong>Capacity:</strong> 1500 mAh</li>
        <li><strong>Energy:</strong> 16.65 Wh</li>
        <li><strong>Discharge Rate:</strong> 35C</li>
        <li><strong>Connector Type:</strong> T-Plug / Deans Connector</li>
        <li><strong>Balance Plug:</strong> JST-XH</li>
        <li><strong>Weight:</strong> ~120g</li>
      </ul>
    </td>
  </tr>
</table>

---

The **Gens Ace 3S Li-Po battery** is a lightweight and powerful energy source designed for high-performance robotics, drones, RC vehicles, and airsoft equipment.  
With a **capacity of 1500mAh** and a **35C discharge rating**, it delivers consistent current output suitable for motors, sensors, and controllers that require stable power.  
Its **compact size and low weight** make it easy to integrate into mobile robotic platforms.  
The built-in **Deans/T-Plug connector** ensures low-resistance power delivery, while the **JST-XH balance connector** allows for safe charging and cell balancing.  

This battery is ideal for applications requiring **high burst currents, reliability, and long cycle life**, making it a solid choice for powering small-to-medium robotics systems.

---

# **Microcontroller :**  
# Raspberry Pi 4 Model B

<table>
  <tr>
    <td>
      <img width="1883" height="1249" alt="image" src="https://github.com/user-attachments/assets/3bcbfe18-2bf9-450b-9099-205b13d61c98" />
    </td>
    <td>
      <h3>Specifications</h3>
      <ul>
        <li><strong>Processor:</strong> Broadcom BCM2711, Quad-core Cortex-A72 (ARM v8) 64-bit SoC @ 1.5GHz</li>
        <li><strong>RAM Options:</strong> 2GB, 4GB, or 8GB LPDDR4-3200 SDRAM</li>
        <li><strong>Networking:</strong> Gigabit Ethernet, 2.4GHz & 5.0GHz 802.11ac Wi-Fi, Bluetooth 5.0</li>
        <li><strong>USB Ports:</strong> 2 √ó USB 3.0, 2 √ó USB 2.0</li>
        <li><strong>Video & Sound:</strong> 2 √ó micro-HDMI ports (up to 4Kp60), MIPI DSI display port, MIPI CSI camera port</li>
        <li><strong>Storage:</strong> microSD card slot for OS and data storage</li>
        <li><strong>GPIO:</strong> 40-pin header, backward-compatible with previous Raspberry Pi boards</li>
        <li><strong>Power:</strong> 5V DC via USB-C connector (minimum 3A)</li>
        <li><strong>Dimensions:</strong> 85.6mm √ó 56.5mm</li>
      </ul>
    </td>
  </tr>
</table>

---

The **Raspberry Pi 4 Model B** is a powerful single-board computer that serves as the main controller of the robot.  
It offers significant improvements over previous models, including a faster **quad-core ARM Cortex-A72 CPU**, more memory options, dual micro-HDMI outputs supporting **dual 4K displays**, and faster networking with **true Gigabit Ethernet** and **dual-band Wi-Fi**.  
Its 40-pin GPIO header provides compatibility with a wide range of sensors, motor drivers, and other modules, making it extremely versatile for robotics and IoT applications.  

Thanks to its **USB 3.0 ports**, it can interface with high-speed peripherals such as cameras, LIDAR sensors, or external storage, while the **USB-C power input** ensures stable operation.  
In this project, the Raspberry Pi 4B acts as the **central brain**, handling computer vision, path planning, and high-level decision-making, while coordinating with microcontrollers, sensors, and actuators to control the entire robot.

---

## **GPIO PINOUT OF RPI 4B** : 


# Raspberry Pi 4 Model B GPIO (40-pin Header)

<table>
  <tr>
    <td>
      <img width="1536" height="954" alt="image" src="https://github.com/user-attachments/assets/4aea45b9-b590-47f6-ac44-85fbfb811bca" />
    </td>
    <td>
      <h3>Specifications</h3>
      <ul>
        <li><strong>Total Pins:</strong> 40 (2√ó20 header)</li>
        <li><strong>Power Pins:</strong> 2 √ó 5V, 2 √ó 3.3V, 8 √ó Ground (GND)</li>
        <li><strong>GPIO Pins:</strong> 26 (configurable as input/output)</li>
        <li><strong>Dedicated Interfaces:</strong> 
          <ul>
            <li>UART (TXD, RXD)</li>
            <li>SPI (MOSI, MISO, SCLK, CE0, CE1)</li>
            <li>I¬≤C (SDA, SCL)</li>
            <li>PWM channels</li>
          </ul>
        </li>
        <li><strong>Voltage Levels:</strong> 3.3V logic (NOT 5V-tolerant)</li>
        <li><strong>Backward Compatibility:</strong> Same pinout as Raspberry Pi 3B/3B+</li>
      </ul>
    </td>
  </tr>
</table>

---

The **Raspberry Pi 4B GPIO header** provides 40 pins that allow direct hardware interfacing with sensors, actuators, and external modules.  
Among these are multiple **power pins (5V and 3.3V)**, **ground pins**, and **26 flexible GPIO pins** which can be programmed as input or output using libraries such as **RPi.GPIO** or **GPIO Zero**.  

The header also exposes dedicated hardware interfaces, including **UART for serial communication**, **SPI for high-speed device connections**, and **I¬≤C for multi-sensor integration**, along with PWM support for driving **motors, LEDs, and servos**.  

Because the pins operate at **3.3V logic levels**, external level shifters are required when interfacing with 5V components.  
This GPIO header is the bridge between the Raspberry Pi and the physical world, making it an essential feature for robotics, IoT, and embedded system projects.

---
---

## **BUCK CONVERTER** (8.4 TO 5V) : 
### XL4016 DC-DC Step-Down Module

<table>
  <tr>
    <td>
      <img width="1000" height="661" alt="image" src="https://github.com/user-attachments/assets/85deceb0-6c83-4362-b530-df1353a46600" />
    </td>
    <td>
      <h3>Specifications</h3>
      <ul>
        <li><strong>Chipset:</strong> XL4016E1 DC-DC Buck Converter</li>
        <li><strong>Input Voltage:</strong> 8V ‚Äì 36V DC</li>
        <li><strong>Output Voltage:</strong> 1.25V ‚Äì 32V DC (adjustable)</li>
        <li><strong>Output Current:</strong> Up to 8A (continuous), 12A peak</li>
        <li><strong>Conversion Efficiency:</strong> Up to 95%</li>
        <li><strong>Switching Frequency:</strong> ~180 kHz</li>
        <li><strong>Protections:</strong> Over-current, thermal shutdown</li>
        <li><strong>Heat Dissipation:</strong> Integrated heatsink for stability under high load</li>
        <li><strong>Dimensions:</strong> 60mm √ó 52mm √ó 20mm (approx.)</li>
      </ul>
    </td>
  </tr>
</table>

---

The **XL4016 buck converter** is a high-power DC‚ÄìDC step-down module designed to provide stable and efficient voltage regulation for robotics, automation, and embedded systems. With an input range of 8‚Äì36V and an adjustable output from 1.25‚Äì32V, it is highly versatile and can power a wide variety of subsystems from a single battery source. Its ability to deliver up to **8A of continuous current** (with peaks of 12A) makes it suitable for driving motor controllers, Raspberry Pi boards, and other current-hungry electronics simultaneously. The module achieves up to 95% efficiency, reducing energy losses and minimizing heat buildup. The integrated heatsink ensures stable operation even under heavy loads, while built-in protection circuits safeguard against over-current and thermal issues. For robotics applications, the XL4016 is often recommended because it allows a single Li-Po or Li-ion battery pack to be safely stepped down to multiple regulated voltages, ensuring reliable operation of both logic-level electronics and power-hungry actuators.



---

## **Sensor :**  
# MPU-6050 6-Axis Gyroscope and Accelerometer

<table>
  <tr>
    <td>
      <img width="535" height="677" alt="image" src="https://github.com/user-attachments/assets/9a91676b-a7fa-4c22-8b1e-426d473f775b" />
    </td>
    <td>
      <h3>Specifications</h3>
      <ul>
        <li><strong>Chipset:</strong> InvenSense MPU-6050</li>
        <li><strong>Sensor Type:</strong> 3-axis gyroscope + 3-axis accelerometer</li>
        <li><strong>Operating Voltage:</strong> 3V ‚Äì 5V</li>
        <li><strong>Communication Interface:</strong> I¬≤C (default address 0x68)</li>
        <li><strong>Gyroscope Range:</strong> ¬±250, ¬±500, ¬±1000, ¬±2000 ¬∞/s</li>
        <li><strong>Accelerometer Range:</strong> ¬±2g, ¬±4g, ¬±8g, ¬±16g</li>
        <li><strong>Sampling Rate:</strong> Up to 1 kHz</li>
        <li><strong>Digital Motion Processor (DMP):</strong> Built-in for sensor fusion</li>
        <li><strong>Dimensions:</strong> ~20mm √ó 15mm √ó 2mm (module size varies)</li>
      </ul>
    </td>
  </tr>
</table>

---

The **MPU-6050** is a 6-axis motion tracking sensor that integrates a **3-axis gyroscope** and a **3-axis accelerometer** on a single chip, making it one of the most widely used sensors in robotics and embedded systems. It communicates via the I¬≤C protocol, which allows it to connect easily to microcontrollers like the Arduino, ESP32, or Raspberry Pi. The sensor provides motion data across multiple selectable ranges, with gyroscope sensitivity from ¬±250 to ¬±2000 degrees per second and accelerometer ranges from ¬±2g to ¬±16g. This versatility makes it suitable for detecting orientation, angular velocity, and linear acceleration in real time. The built-in **Digital Motion Processor (DMP)** can handle sensor fusion internally, combining gyro and accelerometer data to output stable orientation values, reducing the computational load on the main controller. For robotics, the MPU-6050 is commonly used in balancing robots, drones, navigation systems, and motion-controlled projects, where accurate and responsive motion sensing is essential for stability and control.

---

## **Camera :**  
# SJCAM C200 Action Camera

<table>
  <tr>
    <td>
      <img width="268" height="471" alt="image" src="https://github.com/user-attachments/assets/cd978e9f-b1fa-4f9d-8e7e-16a7fe57d2f6" />
    </td>
    <td>
      <h3>Specifications</h3>
      <ul>
        <li><strong>Sensor:</strong> 4K CMOS image sensor</li>
        <li><strong>Video Resolution:</strong> 4K @ 24fps, 2K @ 30fps, 1080p @ 60fps</li>
        <li><strong>Photo Resolution:</strong> Up to 16MP</li>
        <li><strong>Lens:</strong> 154¬∞ wide-angle lens</li>
        <li><strong>Display:</strong> 1.3-inch front screen + status indicators</li>
        <li><strong>Battery:</strong> 1200mAh (removable, USB-C charging)</li>
        <li><strong>Connectivity:</strong> Wi-Fi, USB-C</li>
        <li><strong>Storage:</strong> MicroSD card up to 128GB</li>
        <li><strong>Waterproof:</strong> Up to 5m without case, 30m with case</li>
        <li><strong>Dimensions:</strong> ~68mm √ó 32mm √ó 26mm</li>
      </ul>
    </td>
  </tr>
</table>

---

The **SJCAM C200** is a compact 4K action camera designed for high-quality video and photo capture in robotics, drones, and field-testing environments. It features a **154¬∞ wide-angle lens**, making it well-suited for wide field-of-view applications such as navigation, mapping, and vision-based obstacle detection. The camera supports recording at multiple resolutions, including **4K at 24fps** and **1080p at 60fps**, giving flexibility between image detail and frame rate depending on the task. With a removable **1200mAh battery** and **USB-C charging**, it provides convenient power management, while Wi-Fi connectivity allows real-time preview and data transfer to companion devices. Its rugged design makes it waterproof up to 5m without housing and up to 30m with an additional case, ensuring reliable performance in outdoor and challenging environments. For robotics projects, the SJCAM C200 is an excellent choice as a vision module, balancing portability, durability, and high-definition imaging in a small and versatile package.
---
---

# *PCB*  
## Previous PCB Design and Power Distribution  

| **Top View of PCB** | **Bottom View of PCB** | **Power Management Diagram** |
|----------------------|------------------------|-------------------------------|
| <img src="https://github.com/user-attachments/assets/be051834-2c23-495e-9aa1-83d9620e1524" width="400"/> | <img src="https://github.com/user-attachments/assets/45d68411-f99a-4ef4-8aef-c63eb856a8e1" width="400"/> | <img src="https://github.com/user-attachments/assets/0f01372c-1cff-4ff7-b65b-5e5a5ca5f" width="400"/> |

---

The PCB shown above was designed as the **central power management and distribution unit** for the robot, ensuring that every subsystem received the correct voltage and current for stable operation. The board draws power from two **Lithium-Ion cells**, each rated at 4.2V, providing a combined supply of **8.4V** when fully charged. This input is then carefully regulated using multiple DC‚ÄìDC converters to meet the diverse requirements of the system.  

At the heart of the regulation stage, an **XL4016 buck converter** steps the 8.4V battery voltage down to a stable **5V rail**, which powers the **Raspberry Pi 5** and other core electronics. Additional buck converter modules are integrated to provide subsystem-specific voltages: one module regulates to **6V for the servo motor**, while another supplies **5V dedicated to the ESP32 microcontroller**, ensuring isolation and consistent operation. For the high-power motors, the PCB incorporates a **buck-boost converter** that outputs **12V**, guaranteeing reliable torque and speed even as the battery voltage drops during operation.  

This combination of regulated rails allows the robot to operate multiple controllers, sensors, and actuators from a single compact PCB without instability or power loss. By integrating switching converters and carefully routing the distribution, the board minimizes heat, maximizes efficiency, and protects sensitive comp
onents from voltage fluctuations. Overall, this design highlights the importance of structured power management in robotics, where both logic-level devices and power-hungry actuators must coexist seamlessly on the same platform.

---
# *CurrentPCB* :
<img width="960" height="1280" alt="image" src="https://github.com/user-attachments/assets/3ddcdbbd-1733-4c6f-bdf9-c9cbbeae69e3" />

Compared to our earlier design, the new PCB introduces several improvements aimed at stability, modularity, and ease of use. The earlier version primarily served as a basic power distribution board with buck converters and minimal control features. In this revised version, additional attention has been given to **structured wiring via ribbon connectors**, improved **switching and manual control through a larger rocker and push button**, and **better integration of the buck converter section** with heat dissipation. The placement of capacitors, inductor coil, and screw terminals has been optimized for cleaner power flow, and the routing now reduces noise and voltage drops across the board. Overall, the new design is more robust, reliable, and serviceable compared to the prototype.  

# *CIRCUITDIAGRAM* :
<img width="768" height="1280" alt="image" src="https://github.com/user-attachments/assets/fe85c0f9-0318-444e-8400-203f2f47621a" />

# Explanation:

This circuit integrates the Raspberry Pi GPIO header with a set of sensors, actuators, and motor driver modules, forming the control and power distribution system for a small robot. The Raspberry Pi acts as the central controller, sending signals to motors, servos, and indicators while also receiving sensor feedback to make decisions in real time.
At the top of the diagram, we see the Raspberry Pi GPIO pinout, which is the core interface between the Pi and external devices. Each pin can be configured as an input or output, supplying logic-level signals (3.3V) that control the connected hardware. The Pi provides 5V and 3.3V power rails, as well as ground connections, which are distributed to all modules in the circuit.
A servo motor is connected to the Pi using three wires: VCC (power), GND (ground), and the SIGNAL pin, which is linked to one of the Pi‚Äôs PWM-enabled GPIO pins. This allows the Pi to control the servo angle precisely using PWM (Pulse Width Modulation). Servos are typically used for steering mechanisms or angular adjustments in robots, so this one is likely managing the robot‚Äôs front wheel alignment or a pan/tilt system.
On the left side of the circuit, a push button and an LED are wired into the GPIO header. The push button is used as a digital input ‚Äî when pressed, it connects the GPIO pin to ground, sending a low signal to the Pi. This can be used for resets, mode changes, or manual overrides. The LED, on the other hand, is controlled through a GPIO pin as an output. It provides visual feedback, such as power status, errors, or activity indicators, allowing the operator to easily monitor the robot‚Äôs state.
At the bottom, the L293D motor driver IC is connected between the Pi and a DC motor. The Pi itself cannot supply enough current to directly drive motors, so the L293D acts as a buffer and amplifier. It uses the low-current logic signals from the Pi to control the higher-current flow from the battery to the motor. This enables bidirectional motor control (forward and backward rotation) and also supports PWM for speed regulation. In this configuration, the motor shown is most likely one of the drive motors that moves the robot‚Äôs wheels.
On the right side, we see an MPU-6050 IMU (Inertial Measurement Unit) connected via I¬≤C communication. The SDA (data) and SCL (clock) lines run from the Pi‚Äôs GPIO header to the sensor module, with VCC and GND providing power. The MPU-6050 combines a 3-axis gyroscope and a 3-axis accelerometer, which together measure angular velocity and linear acceleration. This data is essential for balance, orientation, and navigation, particularly in robots that need to track their motion or maintain stability on uneven terrain.
Next to it is a buck converter module (with a heatsink and inductor coil). This component regulates the battery voltage down to safe levels required by the Raspberry Pi, motors, and sensors. Since motors can cause voltage fluctuations when they start or stop, the buck converter ensures that the Pi always receives a clean and stable supply. The input of the buck converter comes directly from the main battery (Li-ion cells), and the outputs are distributed to different subsystems: 5V for the Pi and ESP32, 6V for the servo, and 12V (via buck-boost regulation) for the motors.
The wiring shows how all the power rails (red for positive, black for ground, and green/yellow/orange for signals) are carefully routed to prevent noise and maintain clear separation between logic-level signals and high-current motor wiring. The Pi orchestrates everything: it reads the sensor data from the MPU-6050, processes input from the push button, controls the LED for feedback, drives the servo motor with PWM, and commands the DC motor through the L293D motor driver.

---
# *SCHEMATIC FOR THE NERDS* : 
<img width="729" height="463" alt="image" src="https://github.com/user-attachments/assets/b49c9fba-4add-4737-b6da-5110258971ce" />



###  Camera Placement and Functionality

The robot's main camera is positioned at the top and angled slightly downwards. This setup enhances object detection capabilities by providing:
- **üîç Close-Range Detection**: The camera can identify objects in close proximity with high accuracy.
- **üåê Extended-Range Detection**: Ensures objects further away are detected effectively.

The camera feeds data to the **Raspberry Pi 4**, which processes image recognition algorithms to detect towers and corner lines. The processed data is then transmitted to the **ESP32 microcontroller** for real-time navigation and obstacle avoidance.

---



>[!IMPORTANT]
> **Power Highlights:**
> - The XL4016 Buck Converter ensures stable voltage regulation, critical for protecting the Raspberry Pi and ESP32 microcontroller during operation.
> - Independent buck modules handle the servo motor and ESP32 power needs, optimizing energy usage across all components.

---
---

## Power Regulation and Control PCB

This PCB is a custom-made prototyping board that acts as the main power regulation and control hub of the robot. It integrates a red rocker switch as the primary ON/OFF control, a large red push button for reset or emergency stop functions, and a green LED that indicates when the system is active. Flat ribbon cables connect the board to the Raspberry Pi or Arduino, carrying essential GPIO signals. The lower section features a copper coil, multiple electrolytic capacitors, and MOSFETs/regulators that form a DC‚ÄìDC converter, ensuring stable power delivery from the battery to different subsystems. Blue trimpots allow precise voltage calibration, while a screw terminal block provides secure connections for input and output wiring. Altogether, this PCB manages distribution, stabilization, and manual control of power, making it a central part of the robot‚Äôs electronic system.

![PCB Overview](dc381c7b-e720-4e5c-8d6c-acde376fd400.png)


---




#  OBSTACLE MANAGEMENT AND CODE EPXLANATION

In **Round 1**, our robot **SMOKI** must autonomously complete **three laps** on a predefined track without the need for obstacle avoidance. To achieve precise navigation and lap counting, we have developed a robust algorithm that integrates image processing with control systems.

---

###  Algorithm Overview

1. **üì∏ Image Acquisition**:
   - The robot captures real-time images of the track using its onboard camera.

2. **üé® Color Space Conversion**:
   - Captured images are converted from the **RGB** color space to the **HSV (Hue, Saturation, Value)** color space.
   - The HSV color space is chosen for its effectiveness in color segmentation, as it is less sensitive to lighting variations.

3. **üîç Color Segmentation and Orientation Determination**:
   - Using predetermined HSV ranges, the algorithm isolates the **blue** and **orange** line segments on the track.
   - During the first run, the robot checks whether the blue line appears before the orange line or vice versa.
   - **Orientation Determination**:
     - If the blue line comes before the orange line, the robot sets its orientation accordingly.
     - This initial orientation check ensures the robot follows the track in the correct direction.

4. **üìè Line Detection and Lap Counting**:
   - The **Hough Line Transform** method is applied to detect lines within the segmented images.
   - The robot counts the detected lines to keep track of laps.
   - **Lap Completion**:
     - After counting **12 lines** (corresponding to three laps), the robot initiates a predetermined delay and then stops.

5. **‚öôÔ∏è Position Correction Using PID Control**:
   - Before setting the orientation, a **PID (Proportional-Integral-Derivative) controller** calculates the error.
   - **Error Calculation**:
     - The error is determined by the difference between the distances measured by sensors on the left and right sides of the robot (i.e., `error = left_distance - right_distance` or vice versa).
   - **Distance Maintenance**:
     - If the orientation is **right-based**, the robot maintains a **25 cm** distance from the right side.
     - If the orientation is **left-based**, it maintains a **25 cm** distance from the left side.
   - The PID controller adjusts the robot's steering to minimize the error, helping it stay centered on the track.

---

The process begins with image acquisition and preprocessing, where high-resolution images are captured at regular intervals to provide up-to-date visual data of the track. These images are then converted into the HSV (Hue, Saturation, Value) color space. Unlike RGB, HSV separates brightness from color information, which makes it more effective for thresholding and detecting specific colors under varying lighting conditions.
Once the images are converted, color segmentation is performed. By applying HSV thresholding, the system isolates pixels belonging to the blue and orange ranges. This allows the robot to detect both blue and orange track lines. The sequence of detected colors is also analyzed to establish the robot‚Äôs orientation at the start, ensuring it does not misinterpret direction and mistakenly count laps in reverse.
After segmentation, the system proceeds to line detection using the Hough Transform. First, preprocessing methods such as Gaussian blur and Canny edge detection are applied to enhance the visibility of edges in the image. The Hough Line Transform then detects straight lines by mapping points from image space into a parameter space, identifying lines where multiple points converge. Each detected line crossing increments a counter, and lap completion is recognized after twelve line crossings, which correspond to three full laps of both blue and orange lines combined.
To maintain correct alignment on the track, the robot employs a PID (Proportional‚ÄìIntegral‚ÄìDerivative) control system for position correction. Distance sensors on either side of the robot provide real-time measurements of its relative position. The difference between left and right readings produces an error signal, which the PID controller minimizes. The proportional term reacts to the current error, the integral term compensates for accumulated past errors, and the derivative term predicts future error based on its rate of change. The controller‚Äôs output is translated into steering adjustments, keeping the robot centered. Depending on its orientation, the robot may also bias its movement slightly toward the left or right side of the track, ensuring consistent lap recognition and stable navigation.


## üìä Round 1 Algorithm - Corner Detection Navigation

In Round 1 of the **Future Engineers** category, our robot follows a structured pipeline to complete a lap, detect corners, and terminate correctly at the finish line. The algorithm is designed to balance **robust line tracking** with **precise stopping conditions**.

---
# Explanation of the the first round :
The flowchart represents the logic behind a line-following robot with LIDAR support, where lap counting and orientation checks are integrated into its navigation system. Let‚Äôs walk through it in sequence.
1. Start and Image Capture
The process begins at the Start node. The robot immediately moves to capture an image from its camera. This image forms the basis for detecting whether the guiding line (blue or orange track line) is present in the current frame.
2. Checking for Line Detection
After capturing the image, the robot checks: Is a line detected?
If Yes, it proceeds to the orientation check.
If No, the robot relies on LIDAR scanning to infer the path instead.
This dual check ensures that if the camera fails to detect a line (due to noise, occlusion, or lighting issues), the LIDAR system provides redundancy.
3. Line Detected Path
If a line is found, the robot must determine whether its orientation has already been set.
If orientation is set, the system simply increments the line counter (Line Count ++), tracking how many lines have been crossed.
If orientation is not set, the robot fixes its orientation based on the detected color sequence (blue vs. orange) and also increments the line counter. This prevents mistakes in lap counting by ensuring the robot knows which direction it is traveling.
4. Lap Completion Check
Once the line count is updated, the system checks three conditions simultaneously:
LineCount ‚â• 12 ‚Üí meaning the robot has crossed enough lines for three full laps (each lap includes multiple blue/orange crossings).
LastLine > 1500 ms ‚Üí a time gap condition that prevents double-counting if two detections happen too close together.
FloorDist < 1500 ‚Üí a distance-based condition to ensure the robot is physically on the track surface and not falsely detecting lines elsewhere.
If all conditions are satisfied, the robot concludes that laps are complete, moves to Stop Bot, and terminates the code.
If not, the system moves to path correction with LIDAR and PID control.
5. No Line Detected Path
If the camera does not detect a line, the robot initiates a LIDAR scan of its surroundings. From this scan, it checks the slope of adjacent lines detected by LIDAR.
If the slope is perpendicular, this indicates the robot has crossed a track line, so the line counter is incremented and lap completion is re-evaluated.
If the slope is not perpendicular, the system assumes the robot is simply navigating without crossing a line and moves to LIDAR-based pathfinding.
This branch ensures the robot doesn‚Äôt lose track of lap progress even if the camera fails.
6. Weighted Average Pathfinding with LIDAR
When either:
the lap is not complete, or
no perpendicular crossing is found,
the system falls back to LIDAR-based navigation. Here, the robot uses a weighted average method from LIDAR readings to determine the best safe path forward (e.g., avoiding obstacles and staying centered).
7. PID Steering Correction
Once the best path is determined, the robot applies a PID controller to calculate the precise steering angle.
The error signal is generated from differences in left/right LIDAR (or distance sensor) readings.
The PID controller minimizes this error to keep the robot aligned on track.
8. Continuing Navigation
Finally, the robot applies the PID steering adjustment and continues navigating along the track until it loops back to capture the next image, repeating the cycle until the termination condition is met.
---
**Flowchart:**

```mermaid
flowchart TD
    A[Start] --> B[Capture Image]
    B --> C{Line Detected?}
    
    %% Line Detected Branch
    C -->|Yes| D{Orientation Set?}
    D -->|Yes| E[Line Count ++]
    D -->|No| F[Fix Orientation for Next Run and Line Count ++]
    E --> G{LineCount >= 12 AND LastLine > 1500ms AND FloorDist < 1500?}
    F --> G
    G -->|Yes| H[Stop Bot and Terminate Code]
    G -->|No| K[Use Weighted Average from LIDAR to Find Best Path]
    
    %% No Line Detected Branch
    C -->|No| I[LIDAR Scan Image]
    I --> J{Slope of Adjacent Lines?}
    J -->|Perpendicular| L[Line Count ++]
    J -->|Not Perpendicular| K
    L --> G
    
    %% Common Path
    K --> M[PID Calculates Steering Value]
    M --> N[Continue Navigation]


```



---

# CODE EXPLANATION :
## LIDAR CODE:
<img width="563" height="583" alt="image" src="https://github.com/user-attachments/assets/5ce2c084-b0d3-4706-a306-d9f61da806ca" />

The LidarReader class provides a simple interface for capturing and storing LiDAR sensor data in real time during the robot‚Äôs first round of operation. When initialized, it starts an external LiDAR driver program as a subprocess, configured with a serial port and baud rate. A background thread continuously reads the sensor‚Äôs live output from the subprocess, line by line. Each line, containing an angle and a distance measurement, is parsed using a regular expression. The parsed results are stored in a dictionary where each angle (in degrees) is mapped to its corresponding distance value. This allows the robot to quickly access either the distance at a specific angle or retrieve all available readings at once. The stop() method cleanly terminates the subprocess when the LiDAR is no longer needed. Overall, this design ensures non-blocking, thread-safe data collection from the LiDAR, enabling the robot to maintain a constantly updated map of its surroundings during navigation.
---

## Line Checker Code:
<img width="882" height="583" alt="image" src="https://github.com/user-attachments/assets/027f9e47-5ab8-47d5-9805-eb360d579841" />
<img width="751" height="431" alt="image" src="https://github.com/user-attachments/assets/8ed9272b-93c3-4f1a-ae6b-212d4c8be664" />

# Geometric Line Operations

This repository contains a set of Python functions for performing geometric operations on lines. These functions are useful for tasks like checking if points lie on a straight line, determining if two lines are perpendicular, and finding the intersection of two lines. The operations rely on the `numpy`, `scipy`, and `shapely.geometry` libraries for numerical and geometric computations.

The functions include:

- **`coords(r, theta_deg)`**: Converts polar coordinates (radius `r` and angle `theta_deg` in degrees) into Cartesian coordinates (`x`, `y`). This is useful for converting data in polar form to standard Cartesian coordinates for easier geometric calculations.
  
- **`linearity(points)`**: This function checks if a set of points lies approximately on a straight line using linear regression. The function calculates the best-fit line using the `scipy.stats.linregress` function, returning a boolean indicating if the points are linear (based on an R-squared threshold), the slope and intercept of the line, and the R-squared value of the linear regression. It also handles the case of vertical lines (where all x-coordinates are the same).
  
- **`are_perpendicular(m1, m2)`**: This function checks whether two lines with slopes `m1` and `m2` are perpendicular. For two lines to be perpendicular, the product of their slopes should be `-1`. The function also handles special cases where one or both lines are vertical (undefined slopes), as vertical lines have unique behavior when checking perpendicularity.
  
- **`get_intersection(m0, b0, m1, b1)`**: Given two lines, the function computes their intersection point. The lines are represented by their slopes (`m0`, `m1`) and intercepts (`b0`, `b1`). If one of the lines is vertical (slope is `None`), the function calculates the intersection accordingly. Otherwise, the intersection is found using the formula for the intersection of two lines: solving for `x` and using the result to find `y`.

### Dependencies

The functions rely on the following Python libraries:
- `numpy`: For handling numerical operations such as trigonometry and arrays.
- `scipy`: Specifically, the `scipy.stats.linregress` function is used for performing linear regression and calculating the R-squared value.
- `shapely`: A library for geometric operations (though not directly used in the provided functions, it may be part of future implementations for geometry-related tasks).


# Motor Code :
<img width="559" height="362" alt="image" src="https://github.com/user-attachments/assets/c36a2962-b773-4814-8f58-a7a9d1fa9e92" />


This Python class is designed to control a motor's forward and reverse rotation using the `pigpio` library. It uses PWM (Pulse Width Modulation) to control the speed of the motor, and GPIO pins on the Raspberry Pi are used to drive the motor in either direction. The class provides three primary functions: **forward**, **reverse**, and **stop**.

## Dependencies

- `pigpio`: The `pigpio` library is used to interface with the GPIO pins of the Raspberry Pi. It allows us to control the motor's direction and speed using PWM signals.

- `utility`: This imports `clamp` (and optionally `map_range`) for ensuring the speed value remains within the appropriate range (0 to 255).

## Motor Class

### `__init__(self, pi, forward_pin=12, reverse_pin=18, freq=1000)`

This is the constructor of the `Motor` class. It initializes the motor control, including setting up the GPIO pins and the PWM frequency.

#### Parameters:
- `pi`: The `pigpio.pi()` instance (already connected to the Raspberry Pi).
- `forward_pin`: The GPIO pin number for controlling the forward direction of the motor (default is pin 12).
- `reverse_pin`: The GPIO pin number for controlling the reverse direction of the motor (default is pin 18).
- `freq`: The frequency for the PWM signal (default is 1000 Hz).

### `forward(self, speed=255)`

This function runs the motor in the forward direction at a specified speed. The speed is provided as a PWM duty cycle value ranging from 0 to 255.

#### Parameters:
- `speed`: The motor speed, represented as a PWM duty cycle value. The default value is 255 (maximum speed).

The function:
- Clamps the `speed` to ensure it is between 0 and 255.
- Sets the PWM frequency for the forward direction.
- Sets the duty cycle for the forward direction pin to control the speed.
- Ensures the reverse direction pin is turned off.

### `reverse(self, speed=255)`

This function runs the motor in the reverse direction at a specified speed. Like the `forward()` function, it uses a PWM duty cycle value between 0 and 255 to control speed.

#### Parameters:
- `speed`: The motor speed, represented as a PWM duty cycle value. The default value is 255 (maximum speed).

The function:
- Clamps the `speed` to ensure it is between 0 and 255.
- Sets the PWM frequency for the reverse direction.
- Sets the duty cycle for the reverse direction pin to control the speed.
- Ensures the forward direction pin is turned off.

### `stop(self)`

This function stops the motor completely by turning off both the forward and reverse direction pins. It sets the PWM duty cycle for both pins to 0, effectively stopping the motor.

#### Functionality:
- Sets the PWM duty cycle for both the forward and reverse pins to 0.
- Stops the motor in both directions.

---

# 



# üèÜ Round 2 Algorithm - Lap Completion with Obstacle Avoidance and Object Detection

Round 2 involves an enhanced version of our robot SMOKI, which autonomously completes three laps, avoiding obstacles and calculating the steering value based on object prioritization and boundary detection. Below is an in-depth look at the step-by-step algorithm implemented for this round.

## üåü Step-by-Step Algorithm Overview

### üì∏ Image Acquisition:
- The camera captures real-time images of the track and surroundings.

### üé® HSV Color Conversion:
- Convert the captured image from RGB to HSV (Hue, Saturation, Value) scale.
- Store all relevant line and object colors for further use.

### üí® Gaussian Blur:
- Apply Gaussian Blur to the frame to reduce noise.
- Based on predetermined HSV range, isolate the black border portion.

### ‚ö´ Black Border Detection:
- Use Canny Edge Detection to identify the black border walls.
- Apply Hough Line Probabilistic Transformation to determine acceptable border walls.

### üõë Border Masking:
- Create a border in the frame based on the detected lines.
- Mask out everything beyond the border to eliminate unnecessary information for the rest of the algorithm.

### üîç Line Detection (Blue or Orange):
- Identify lines using the predetermined HSV color range.
- Check for the blue or orange line in the frame.

### üìê Slope Calculation:
- If a line is found, calculate the minimum and maximum slopes for both the blue and orange lines (if both are present).

### ‚Ü©Ô∏è Orientation Determination:
- If both blue and orange lines are detected, compare their slopes.
- Determine the clockwise or anti-clockwise orientation and store this in a variable for future reference.

### üìä Object Detection and Prioritization:
- Detect acceptable objects within the boundary wall.
- Assign a priority value to each object based on distance, orientation, and coordinates.
- Register only one object based on predetermined mathematical calculations.
- Calculate the steering value for navigation using a quadratic function based on the object's distance and position along the x-axis.

### üì° Serial Communication:
- Send the calculated steering value to the ESP32 via serial communication to adjust the robot's movement accordingly.

---

## Algorithm Explanation

- **HSV Conversion** allows for effective color segmentation, making it easier to distinguish between track lines and other features regardless of lighting conditions.
- **Gaussian Blur** helps to reduce noise, making the detection of borders and lines more reliable.
- Using **Hough Line Transform** and **Canny Edge Detection** enables accurate identification of boundaries, which is crucial for masking irrelevant parts of the frame.
- **Slope Comparison** provides the robot with information about its current orientation, enabling it to differentiate between clockwise and anti-clockwise directions based on the detected lines.
- The **Object Prioritization** mechanism ensures that the robot only reacts to the most relevant obstacle, improving navigation efficiency.
- Finally, the calculated steering value is sent to the ESP32 for precise movement control, ensuring that the robot maintains its intended path while avoiding obstacles effectively.

## Video Tutorial for Hough Line Transform

For a better understanding of how the Hough Line Transform method is used in our algorithm, you can watch this detailed video tutorial:

- [üîó Hough Line Transform Tutorial by DigitalSreeni](https://www.youtube.com/watch?v=6-3HgNZkDGA)

## Next Steps

- Test the algorithm in various track conditions to ensure robustness.
- Fine-tune the HSV color ranges and quadratic function for steering to achieve optimal performance.
- Collect data on the robot's performance to further refine the object prioritization logic.

Feel free to reach out if you need more insights or help with further tuning the algorithm!




## **Flowchart**

```mermaid

flowchart TD
    A[Start] --> B[Capture Image]
    B --> O{Object Detected?}

    %% Object Detected Branch
    O -->|Red Object| P[Steer Right with PID]
    O -->|Green Object| Q[Steer Left with PID]
    P --> N[Continue Navigation]
    Q --> N

    %% No Object Detected Branch ‚Üí Round 1 Logic
    O -->|No| C{Line Detected?}
    
    %% Line Detected = Yes
    C -->|Yes| D{Orientation Set?}
    D -->|Yes| E[Line Count ++]
    D -->|No| F[Fix Orientation for Next Run and Line Count ++]
    E --> G{LineCount >= 12 AND LastLine > 1500ms AND FloorDist < 1500?}
    F --> G
    G -->|Yes| H[Initiate Parking Maneuver]
    G -->|No| K[Use Weighted Average from LIDAR to Find Best Path]

    %% Line Detected = No
    C -->|No| X{LineCount >= 12 AND LastLine > 1500ms AND FloorDist < 1500?}
    X -->|Yes| H
    X -->|No| I[LIDAR Scan Image]
    I --> K
    
    %% Common Path
    K --> M[PID Calculates Steering Value]
    M --> N[Continue Navigation]
```
<p align="center">
  # THATS ALL FROM US
</p>


<img src="https://github.com/user-attachments/assets/3c4eb0aa-37f5-4255-a176-253f5ae422f5" />
