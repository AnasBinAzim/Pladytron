# Problems Faced and Solutions Implemented :
1. One of the first challenges we encountered was with the SJCAM C200 action camera. Our goal was to use it as the primary vision system for the robot, automatically starting up in action camera mode at power-on so that it could immediately begin capturing video. However, the camera’s firmware design posed a significant obstacle: in order to activate recording mode, the user is required to press two buttons in sequence. Since our robot’s physical interface only allows automation of a single power or trigger button, this created a serious limitation. In a robotics application where everything should initialize seamlessly on startup, having to manually press multiple buttons was simply not acceptable. After repeated trials and attempts to work around this behavior, it became clear that the SJCAM C200 could not meet our requirements in its default form.
The solution was drastic but effective: we removed the action camera role entirely and repurposed the device to function strictly as a PC camera on startup. By rewiring the system and bypassing the dual-button activation sequence, we ensured that the camera would automatically initialize in webcam mode as soon as power was applied. This change eliminated the need for manual intervention and brought the system into alignment with the principle of full automation. Although it meant giving up native onboard recording features, the trade-off was worthwhile because it provided reliable, hands-free video capture directly integrated with the rest of the robot’s processing pipeline. This decision not only streamlined startup but also reduced potential points of failure in field deployment.

# CAMERA BEFORE AND AFTER :



| **Before (SJCAM C200 Action Camera Mode)** | **After (Rewired for Auto PC Camera Mode)** |
|---------------------------------------------|---------------------------------------------|
|<img width="600" height="600" alt="image" src="https://github.com/user-attachments/assets/9dba0dc6-bd10-484f-b9a8-f090cf1020c0" /> | <img width="202" height="321" alt="image" src="https://github.com/user-attachments/assets/09362b7b-ded8-45f0-9df3-c3d53b894797" />|



2. A second major issue arose from our initial reliance on ultrasonic sonar sensors for distance measurement and obstacle detection. Sonar modules are attractive because they are inexpensive, lightweight, and widely available. However, during testing we discovered several critical shortcomings. Low-cost sonar sensors proved to be highly unreliable, with inconsistent readings that were easily disrupted by soft materials, angled surfaces, or environmental noise. Furthermore, using multiple sonar units in parallel introduced cross-interference, leading to false echoes and data corruption. The wiring complexity increased with each additional module, and integrating them into the PCB led to higher chances of error, noise coupling, and troubleshooting overhead. For a robot that demands precision, these limitations quickly became unacceptable.
To address this, we transitioned from sonar sensors to a LiDAR-based solution. Unlike sonar, LiDAR provides much greater coverage with a single sensor, reducing both hardware complexity and software overhead. With LiDAR, the robot was able to generate accurate distance measurements over a wide field of view, allowing it to map its environment with higher confidence and fewer blind spots. This not only simplified the PCB layout by removing multiple unreliable sonar channels but also improved system reliability by eliminating the issues of echo interference and inconsistent readings. The result was a cleaner, more efficient sensing system that required less debugging while dramatically improving environmental awareness and navigation capabilities.
Together, these problem-solving decisions reflect the iterative engineering process behind the robot. Each limitation encountered — from the camera’s startup logic to the sonar sensor unreliability — forced us to rethink the design and implement more robust alternatives. By replacing manual-dependent hardware with automated solutions and swapping error-prone sensors for advanced LiDAR technology, we made the robot more reliable, maintainable, and capable of performing in real-world conditions.
<img width="960" height="1092" alt="image" src="https://github.com/user-attachments/assets/38ceee90-687d-409c-8e0a-09ea321bbc57" />

In our earlier designs, we relied on inexpensive ultrasonic sonar sensors for obstacle detection and distance measurement. While attractive due to their low cost and simple wiring, these sensors quickly revealed major drawbacks in practice. Ultrasonic sonar works by sending out sound waves and measuring the echo, but this method is highly susceptible to error. Surfaces that are angled or soft (like fabric, foam, or carpet) often fail to reflect sound properly, resulting in blind spots or incorrect distance readings. Even when multiple sensors were used, they interfered with one another, creating cross-talk and producing false data. This meant that instead of gaining reliable environmental awareness, we were constantly troubleshooting noisy signals and inconsistent measurements. The more sonar modules we added, the more complex and error-prone the PCB wiring became, increasing both hardware overhead and debugging effort.
By contrast, the RPLIDAR C1 provides a laser-based scanning solution that is far more precise, reliable, and scalable. Instead of relying on slow and noisy sound echoes, the RPLIDAR uses a laser beam to measure distances with millimeter-level accuracy, scanning a full 360° field of view at high speed. This gives the robot comprehensive coverage of its surroundings with a single sensor, eliminating the need for an array of multiple sonars pointing in different directions. With the RPLIDAR C1, mapping and navigation become far simpler because the data is consistent, has a higher refresh rate, and integrates directly into SLAM (Simultaneous Localization and Mapping) algorithms. Unlike sonar, it is not easily confused by material softness, surface angles, or environmental noise.
Another major advantage is range and density of data. Cheap sonar sensors typically measure reliably only within 2–4 meters, and often with wide, imprecise beams that blur object edges. The RPLIDAR C1, on the other hand, provides coverage up to 12 meters with thousands of distance points per second, creating a detailed map of the robot’s environment. This higher density of data not only improves navigation but also enables advanced behaviors such as path planning, obstacle avoidance, and dynamic re-routing in real time. For robotics projects that require autonomy and reliability, these differences are critical.
Finally, the use of RPLIDAR reduces design complexity. Instead of wiring and calibrating multiple sonar units, the robot only needs a single LiDAR sensor connected via USB or UART. This simplifies the PCB layout, minimizes error sources, and improves overall system reliability. While the upfront cost of RPLIDAR C1 is higher than a set of cheap sonars, the trade-off in performance, reliability, and maintainability makes it a far better choice for serious robotics applications.
In summary, while sonar sensors are useful for very simple distance measurements, they quickly become a bottleneck in robotics projects due to unreliability and complexity. The RPLIDAR C1 provides accurate, wide-range, 360° scanning with minimal wiring and maximum reliability, making it vastly superior for autonomous navigation and mapping in our robot.
