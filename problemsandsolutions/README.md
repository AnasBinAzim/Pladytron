Problems Faced and Solutions Implemented
One of the first challenges we encountered was with the SJCAM C200 action camera. Our goal was to use it as the primary vision system for the robot, automatically starting up in action camera mode at power-on so that it could immediately begin capturing video. However, the camera’s firmware design posed a significant obstacle: in order to activate recording mode, the user is required to press two buttons in sequence. Since our robot’s physical interface only allows automation of a single power or trigger button, this created a serious limitation. In a robotics application where everything should initialize seamlessly on startup, having to manually press multiple buttons was simply not acceptable. After repeated trials and attempts to work around this behavior, it became clear that the SJCAM C200 could not meet our requirements in its default form.
The solution was drastic but effective: we removed the action camera role entirely and repurposed the device to function strictly as a PC camera on startup. By rewiring the system and bypassing the dual-button activation sequence, we ensured that the camera would automatically initialize in webcam mode as soon as power was applied. This change eliminated the need for manual intervention and brought the system into alignment with the principle of full automation. Although it meant giving up native onboard recording features, the trade-off was worthwhile because it provided reliable, hands-free video capture directly integrated with the rest of the robot’s processing pipeline. This decision not only streamlined startup but also reduced potential points of failure in field deployment.

# CAMERA BEFORE AND AFTER :



| **Before (SJCAM C200 Action Camera Mode)** | **After (Rewired for Auto PC Camera Mode)** |
|---------------------------------------------|---------------------------------------------|
|<img width="600" height="600" alt="image" src="https://github.com/user-attachments/assets/9dba0dc6-bd10-484f-b9a8-f090cf1020c0" /> | <img width="202" height="321" alt="image" src="https://github.com/user-attachments/assets/09362b7b-ded8-45f0-9df3-c3d53b894797" />|



A second major issue arose from our initial reliance on ultrasonic sonar sensors for distance measurement and obstacle detection. Sonar modules are attractive because they are inexpensive, lightweight, and widely available. However, during testing we discovered several critical shortcomings. Low-cost sonar sensors proved to be highly unreliable, with inconsistent readings that were easily disrupted by soft materials, angled surfaces, or environmental noise. Furthermore, using multiple sonar units in parallel introduced cross-interference, leading to false echoes and data corruption. The wiring complexity increased with each additional module, and integrating them into the PCB led to higher chances of error, noise coupling, and troubleshooting overhead. For a robot that demands precision, these limitations quickly became unacceptable.
To address this, we transitioned from sonar sensors to a LiDAR-based solution. Unlike sonar, LiDAR provides much greater coverage with a single sensor, reducing both hardware complexity and software overhead. With LiDAR, the robot was able to generate accurate distance measurements over a wide field of view, allowing it to map its environment with higher confidence and fewer blind spots. This not only simplified the PCB layout by removing multiple unreliable sonar channels but also improved system reliability by eliminating the issues of echo interference and inconsistent readings. The result was a cleaner, more efficient sensing system that required less debugging while dramatically improving environmental awareness and navigation capabilities.
Together, these problem-solving decisions reflect the iterative engineering process behind the robot. Each limitation encountered — from the camera’s startup logic to the sonar sensor unreliability — forced us to rethink the design and implement more robust alternatives. By replacing manual-dependent hardware with automated solutions and swapping error-prone sensors for advanced LiDAR technology, we made the robot more reliable, maintainable, and capable of performing in real-world conditions.
